{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65abc04a",
   "metadata": {},
   "source": [
    "**Name**: Himanshu Aggarwal\n",
    "\n",
    "**Email**: erhimanshuagarwal79@gmail.com\n",
    "\n",
    "**Assignment Name**: Assignment_6_MODULE_8_SVM & Naive Bayes_ML_10102025.ipynb\n",
    "\n",
    "**Phone no.**: 9711783242"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fed87b",
   "metadata": {},
   "source": [
    "### Question 1: What is a Support Vector Machine (SVM), and how does it work?\n",
    "\n",
    "**Answer:**\n",
    "Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression tasks. It works by finding the optimal hyperplane that separates data points of different classes with the maximum margin. The data points closest to the hyperplane are called *support vectors*, and they are critical in defining the decision boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d46bdc6",
   "metadata": {},
   "source": [
    "### Question 2: Explain the difference between Hard Margin and Soft Margin SVM.\n",
    "\n",
    "**Answer:**\n",
    "- **Hard Margin SVM:** Assumes data is linearly separable. It tries to find a hyperplane that perfectly separates the classes without any misclassification. However, it’s sensitive to noise and outliers.\n",
    "- **Soft Margin SVM:** Allows some misclassifications by introducing a penalty term. It balances between maximizing the margin and minimizing the classification error. It’s more robust for noisy or non-linearly separable data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7764d333",
   "metadata": {},
   "source": [
    "### Question 3: What is the Kernel Trick in SVM? Give one example of a kernel and explain its use case.\n",
    "\n",
    "**Answer:**\n",
    "The **Kernel Trick** allows SVM to classify data that isn’t linearly separable by mapping it into a higher-dimensional space where it can be separated linearly. Instead of computing the mapping explicitly, kernel functions compute inner products in this new space efficiently.\n",
    "\n",
    "**Example:** Radial Basis Function (RBF) kernel.\n",
    "\n",
    "Use Case: When data has complex nonlinear relationships, the RBF kernel can capture those patterns effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1dcd3e",
   "metadata": {},
   "source": [
    "### Question 4: What is a Naïve Bayes Classifier, and why is it called “naïve”?\n",
    "\n",
    "**Answer:**\n",
    "Naïve Bayes is a probabilistic classifier based on Bayes’ Theorem. It assumes that the features are conditionally independent given the class label, which simplifies computation. It is called *naïve* because this independence assumption rarely holds true in real-world data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf97bfe7",
   "metadata": {},
   "source": [
    "### Question 5: Describe the Gaussian, Multinomial, and Bernoulli Naïve Bayes variants. When would you use each one?\n",
    "\n",
    "**Answer:**\n",
    "- **Gaussian Naïve Bayes:** Used for continuous data following a normal distribution (e.g., Iris dataset).\n",
    "- **Multinomial Naïve Bayes:** Used for discrete data, such as word counts in text classification (e.g., spam filtering).\n",
    "- **Bernoulli Naïve Bayes:** Used for binary/boolean features (e.g., whether a word appears in an email or not)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52673bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Support Vectors:\n",
      " [[4.8 3.4 1.9 0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.7 3.  5.  1.7]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.  3.  4.8 1.8]\n",
      " [6.  2.2 5.  1.5]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [5.9 3.  5.1 1.8]\n",
      " [4.9 2.5 4.5 1.7]]\n"
     ]
    }
   ],
   "source": [
    "# Question 6: Write a Python program to:\n",
    "# ● Load the Iris dataset\n",
    "# ● Train an SVM Classifier with a linear kernel\n",
    "# ● Print the model's accuracy and support vectors.\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "iris = datasets.load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train SVM\n",
    "svm_clf = SVC(kernel='linear')\n",
    "svm_clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = svm_clf.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Support Vectors:\\n\", svm_clf.support_vectors_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a32eded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.93      0.96        43\n",
      "           1       0.96      1.00      0.98        71\n",
      "\n",
      "    accuracy                           0.97       114\n",
      "   macro avg       0.98      0.97      0.97       114\n",
      "weighted avg       0.97      0.97      0.97       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Question 7: Write a Python program to:\n",
    "# ● Load the Breast Cancer dataset\n",
    "# ● Train a Gaussian Naïve Bayes model\n",
    "# ● Print its classification report including precision, recall, and F1-score.\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load dataset\n",
    "data = load_breast_cancer()\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train model\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = gnb.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa6f6113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'C': 10, 'gamma': 0.001}\n",
      "Accuracy: 0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "# Question 8: Write a Python program to:\n",
    "# ● Train an SVM Classifier on the Wine dataset using GridSearchCV to find the best C and gamma.\n",
    "# ● Print the best hyperparameters and accuracy.\n",
    "\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "wine = load_wine()\n",
    "X_train, X_test, y_train, y_test = train_test_split(wine.data, wine.target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Grid search\n",
    "param_grid = {'C': [0.1, 1, 10], 'gamma': [0.1, 0.01, 0.001]}\n",
    "grid = GridSearchCV(SVC(kernel='rbf'), param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Results\n",
    "print(\"Best Parameters:\", grid.best_params_)\n",
    "y_pred = grid.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676281da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 9: Write a Python program to:\n",
    "# ● Train a Naïve Bayes Classifier on a synthetic text dataset (e.g. using sklearn.datasets.fetch_20newsgroups).\n",
    "# ● Print the model's ROC-AUC score for its predictions.\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load data\n",
    "categories = ['sci.space', 'talk.politics.misc']\n",
    "data = fetch_20newsgroups(subset='all', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "# Split and vectorize\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42)\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "# Train model\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_vec, y_train)\n",
    "\n",
    "# Predict\n",
    "y_proba = nb.predict_proba(X_test_vec)[:, 1]\n",
    "print(\"ROC-AUC Score:\", roc_auc_score(y_test, y_proba))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9674c5f0-eb6f-46ef-9f58-4aa587d00dce",
   "metadata": {},
   "source": [
    "### Question 10: Imagine you’re working as a data scientist for a company that handles email communications.\n",
    "# Your task is to automatically classify emails as Spam or Not Spam. The emails may contain:\n",
    "# ● Text with diverse vocabulary\n",
    "# ● Potential class imbalance (far more legitimate emails than spam)\n",
    "# ● Some incomplete or missing data\n",
    "# Explain the approach you would take to:\n",
    "# ● Preprocess the data (e.g. text vectorization, handling missing data)\n",
    "# ● Choose and justify an appropriate model (SVM vs. Naïve Bayes)\n",
    "# ● Address class imbalance\n",
    "# ● Evaluate the performance of your solution with suitable metrics And explain the business impact of your solution.\n",
    "\n",
    "**Approach:**\n",
    "1. **Preprocessing:**\n",
    "   - Convert text to lowercase and remove stopwords.\n",
    "   - Handle missing data by replacing empty emails with placeholders.\n",
    "   - Use **TF-IDF Vectorization** for text representation.\n",
    "\n",
    "2. **Model Choice:**\n",
    "   - Use **Multinomial Naïve Bayes** for text data due to its efficiency with word frequencies.\n",
    "   - Alternatively, **SVM** can be used if more complex patterns exist, but it is computationally heavier.\n",
    "\n",
    "3. **Handling Class Imbalance:**\n",
    "   - Use techniques such as **SMOTE**, **class weighting**, or **undersampling**.\n",
    "\n",
    "4. **Evaluation Metrics:**\n",
    "   - Use **Precision**, **Recall**, **F1-score**, and **ROC-AUC**.\n",
    "   - Especially focus on recall for spam detection to reduce false negatives.\n",
    "\n",
    "**Business Impact:**\n",
    "Accurate spam detection improves user trust, reduces exposure to phishing and malware, and enhances productivity by keeping inboxes clean."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
